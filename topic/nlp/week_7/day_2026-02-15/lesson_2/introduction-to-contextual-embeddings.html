<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction to Contextual Embeddings</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 24px;
      color: #111827;
      background: #ffffff;
    }
    h1, h2, h3 { color: #111827; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code { background-color: #f3f4f6; padding: 2px 6px; border-radius: 6px; }
    pre { background-color: #0b1020; color: #e5e7eb; padding: 16px; border-radius: 10px; overflow-x: auto; }
    pre code { background: transparent; padding: 0; }
    blockquote { border-left: 4px solid #e5e7eb; margin: 0; padding-left: 16px; color: #4b5563; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { border: 1px solid #e5e7eb; padding: 8px; text-align: left; }
    th { background: #f9fafb; }
    hr { border: none; border-top: 1px solid #e5e7eb; margin: 24px 0; }
  </style>
</head>
<body>
<h1 id="topic-introduction-to-contextual-embeddings">Topic: Introduction to Contextual Embeddings</h1>
<h2 id="1-formal-definition-what-is-it-and-how-can-we-use-it">1) Formal definition (what is it, and how can we use it?)</h2>
<p>Contextual word embeddings are representations of words where the meaning of a word is determined by its surrounding words (the context). Unlike static word embeddings (like Word2Vec or GloVe) which assign a single vector to each word regardless of its usage, contextual embeddings generate different vectors for the same word based on the sentence or document it appears in.</p>
<p>Formally, let <code>w</code> be a word in a sentence <code>S = [w_1, w_2, ..., w_n]</code>. A contextual embedding model <code>f</code> takes the sentence <code>S</code> as input and produces a vector <code>v_i = f(S, i)</code> for the word <code>w_i</code> at position <code>i</code>. This vector <code>v_i</code> represents the contextual meaning of <code>w_i</code> given the sentence <code>S</code>.</p>
<p>We can use contextual embeddings for various downstream tasks, including:</p>
<ul>
<li><strong>Text Classification:</strong> Representing documents or sentences as the average or concatenation of contextual embeddings.</li>
<li><strong>Named Entity Recognition (NER):</strong>  Using the contextual embedding of a word as a feature for classifying its entity type.</li>
<li><strong>Question Answering:</strong>  Embedding questions and passages, then using attention mechanisms to find relevant information based on the context.</li>
<li><strong>Sentiment Analysis:</strong>  Determining the overall sentiment of a sentence by analyzing the contextual embeddings of its words.</li>
<li><strong>Machine Translation:</strong> Improving translation accuracy by considering the context of words in both the source and target languages.</li>
<li><strong>Text Generation:</strong>  Predicting the next word in a sequence based on the contextual embeddings of the preceding words.</li>
</ul>
<h2 id="2-application-scenario">2) Application scenario</h2>
<p>Consider the word "bank." In static word embeddings, "bank" would have a single vector representation. However, "bank" can have different meanings:</p>
<ul>
<li>"I went to the <strong>bank</strong> to deposit money." (Financial institution)</li>
<li>"The river <strong>bank</strong> was overgrown with weeds." (Land alongside a river)</li>
</ul>
<p>With contextual embeddings, the embedding for "bank" in the first sentence would be different from the embedding for "bank" in the second sentence. A model like BERT would capture the semantic difference based on the surrounding words.</p>
<p>Imagine a sentiment analysis task where we want to determine if a sentence expresses a positive or negative sentiment.  The sentence "That was a <strong>sick</strong> performance!"  could be misinterpreted by a model using static word embeddings because "sick" often has a negative connotation. A contextual embedding model, however, would recognize that in this context, "sick" likely indicates something positive or impressive, leading to a more accurate sentiment prediction.</p>
<h2 id="3-python-method-if-possible">3) Python method (if possible)</h2>
<p>We can use the <code>transformers</code> library in Python to generate contextual embeddings using models like BERT, RoBERTa, or DistilBERT.</p>
<pre class="codehilite"><code class="language-python">from transformers import AutoTokenizer, AutoModel
import torch

# Model and tokenizer names
model_name = &quot;bert-base-uncased&quot;  # Or any other pre-trained model

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# Example sentence
sentence = &quot;The river bank was overgrown with weeds.&quot;

# Tokenize the sentence
tokens = tokenizer(sentence, return_tensors=&quot;pt&quot;)

# Get the embeddings
with torch.no_grad():
    outputs = model(**tokens)

# Extract the contextual embeddings (last hidden state)
embeddings = outputs.last_hidden_state

# 'embeddings' is a tensor of shape (batch_size, sequence_length, hidden_size)
# In this case, batch_size is 1 and sequence_length is the number of tokens.
# hidden_size is the dimensionality of the embeddings (e.g., 768 for bert-base-uncased).

# To get the embedding of a specific word, you need to identify its token index.
# For example, to get the embedding of &quot;bank&quot;:
bank_index = tokens[&quot;input_ids&quot;][0].tolist().index(tokenizer.encode(&quot;bank&quot;, add_special_tokens=False)[0])

bank_embedding = embeddings[0, bank_index, :]

# Print the shape of the 'bank' embedding
print(bank_embedding.shape) # Output: torch.Size([768])

# You can now use this 'bank_embedding' for downstream tasks.

#Example with another sentence:
sentence2 = &quot;I went to the bank to deposit money.&quot;
tokens2 = tokenizer(sentence2, return_tensors=&quot;pt&quot;)

with torch.no_grad():
  outputs2 = model(**tokens2)

embeddings2 = outputs2.last_hidden_state
bank_index2 = tokens2[&quot;input_ids&quot;][0].tolist().index(tokenizer.encode(&quot;bank&quot;, add_special_tokens=False)[0])
bank_embedding2 = embeddings2[0, bank_index2, :]

#Check if the embeddings are different:
print(torch.equal(bank_embedding, bank_embedding2)) #output: False
</code></pre>

<h2 id="4-follow-up-question">4) Follow-up question</h2>
<p>How are contextual embeddings typically pre-trained, and what are the advantages and disadvantages of different pre-training objectives (e.g., Masked Language Modeling, Next Sentence Prediction)?</p>
</body>
</html>
