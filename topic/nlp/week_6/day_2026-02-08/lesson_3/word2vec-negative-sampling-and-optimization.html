
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Word2Vec: Negative Sampling and Optimization</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1, h2, h3 { color: #2c3e50; }
        code { background-color: #f8f9fa; padding: 2px 4px; border-radius: 4px; }
        pre { background-color: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }
        blockquote { border-left: 4px solid #eee; margin: 0; padding-left: 15px; color: #666; }
    </style>
</head>
<body>
    <p>Topic: Word2Vec: Negative Sampling and Optimization</p>
<p>1- Provide formal definition, what is it and how can we use it?</p>
<p>Word2Vec is a group of techniques to learn word embeddings, which are vector representations of words in a high-dimensional space. These embeddings capture semantic relationships between words, allowing algorithms to understand context and meaning beyond simple string matching. Word2Vec has two main architectures: Continuous Bag-of-Words (CBOW) and Skip-gram.</p>
<p><strong>Negative Sampling</strong> is an optimization technique for training word embeddings, primarily used with Skip-gram and CBOW models. Instead of updating all word vectors for every training sample, negative sampling only updates a small sample of the weights. This significantly reduces the computational burden, making training feasible on large datasets.</p>
<ul>
<li>
<p><strong>Formal Definition:</strong>  For each observed word pair (center word <code>w_c</code>, context word <code>w_o</code>), treated as a <em>positive</em> example, we generate <em>k</em> random <em>negative</em> samples, denoted as <code>w_i</code>. These negative samples are words from the vocabulary that did <em>not</em> appear in the context of the center word.  The model is then trained to distinguish between the true pair (<code>w_c</code>, <code>w_o</code>) and the randomly sampled negative pairs (<code>w_c</code>, <code>w_i</code>).</p>
</li>
<li>
<p><strong>How it works:</strong></p>
<ul>
<li><strong>Positive Sample:</strong> For a word pair (context, target) observed in the corpus, we aim to maximize the probability:  <code>P(D=1 | w_o, w_c; θ)</code> where D=1 indicates a positive example (real context).  <code>θ</code> represents the model's parameters (word vectors).</li>
<li><strong>Negative Samples:</strong> For each positive sample, we sample <em>k</em> words from a noise distribution <code>P_n(w)</code>.  These represent words that are unlikely to appear as context words for the target word. We aim to minimize the probability: <code>P(D=0 | w_i, w_c; θ)</code> for each negative sample <code>w_i</code>, where D=0 indicates a negative example.</li>
<li><strong>Noise Distribution <code>P_n(w)</code>:</strong>  A common choice for the noise distribution is the unigram distribution raised to the power of 3/4: <code>P_n(w) = U(w)^{3/4} / Z</code>, where <code>U(w)</code> is the unigram frequency of word <em>w</em> in the corpus, and Z is a normalization constant. This favors less frequent words for negative sampling, since frequent words are already likely to be related to many words.</li>
</ul>
</li>
<li>
<p><strong>Loss Function:</strong> The objective function is typically the negative log-likelihood of the training data. After negative sampling, the loss function becomes a sum over the positive and negative samples:</p>
<p><code>J(θ) = -log σ(v_o^T v_c) - Σ_{i=1}^k log σ(-v_i^T v_c)</code></p>
<p>where:
    *   <code>σ</code> is the sigmoid function ( <code>σ(x) = 1 / (1 + exp(-x))</code> )
    *   <code>v_o</code> is the vector representation of the context word.
    *   <code>v_c</code> is the vector representation of the center word.
    *   <code>v_i</code> is the vector representation of the i-th negative sample.
    *   <code>k</code> is the number of negative samples.</p>
</li>
<li>
<p><strong>How we can use it:</strong> By training the Word2Vec model with negative sampling, we obtain high-quality word embeddings. These embeddings can be used in downstream NLP tasks like:</p>
<ul>
<li><strong>Semantic Similarity:</strong> Finding words with similar meanings.</li>
<li><strong>Word Analogies:</strong>  Solving analogies like "king - man + woman = queen".</li>
<li><strong>Text Classification:</strong> Using word embeddings as features for classifiers.</li>
<li><strong>Machine Translation:</strong> Improving translation quality by aligning embedding spaces.</li>
<li><strong>Recommendation Systems:</strong> Recommending items based on semantic similarity of their descriptions.</li>
</ul>
</li>
</ul>
<p>2- Provide an application scenario</p>
<p><strong>Application Scenario: Building a Product Recommendation System</strong></p>
<p>Imagine an e-commerce platform wants to improve its product recommendations. They have a large dataset of product descriptions and user purchase histories.  Instead of relying solely on collaborative filtering (user-item interactions), they want to incorporate semantic information from the product descriptions.</p>
<p>Here's how Word2Vec with negative sampling can be applied:</p>
<ol>
<li><strong>Data Preparation:</strong> The product descriptions are treated as text sequences. Each sentence can be considered as a training example.</li>
<li><strong>Word Embedding Training:</strong> A Word2Vec Skip-gram model is trained on the combined corpus of product descriptions using negative sampling. This produces word embeddings where words describing similar products are close in the embedding space.</li>
<li><strong>Product Embedding Generation:</strong> Each product is represented by the average of the embeddings of the words in its description. This creates a product embedding for each item in the catalog. (Other approaches include using Doc2Vec or similar paragraph embedding techniques for richer embeddings.)</li>
<li><strong>Recommendation Engine:</strong>  When a user views a particular product, the system calculates the cosine similarity between the viewed product's embedding and the embeddings of all other products. Products with high cosine similarity are recommended to the user, as they are semantically similar based on their descriptions.</li>
<li><strong>Hybrid Approach:</strong> This semantic similarity-based recommendation can be combined with traditional collaborative filtering to improve recommendation accuracy and provide more diverse suggestions.  For example, products similar to what other users who bought the viewed product also purchased could be added to the recommendations.</li>
</ol>
<p>In this scenario, negative sampling is crucial because the vocabulary of product descriptions can be very large. Training without negative sampling would be computationally expensive and time-consuming.</p>
<p>3- Provide a method to apply in python</p>
<p>python
import gensim
from gensim.models import Word2Vec
import nltk
from nltk.corpus import brown
from nltk.tokenize import word_tokenize</p>
<h1>Download required NLTK resources if not already present</h1>
<p>try:
    brown.words()
except LookupError:
    nltk.download('brown')
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')</p>
<h1>1. Prepare the Training Data</h1>
<h1>Use the Brown corpus as an example (replace with your own data)</h1>
<p>sentences = brown.sents()</p>
<h1>Tokenize the sentences (if not already tokenized)</h1>
<p>tokenized_sentences = [[word.lower() for word in sentence] for sentence in sentences]</p>
<h1>2. Train the Word2Vec Model with Negative Sampling</h1>
<p>model = Word2Vec(sentences=tokenized_sentences,
                 vector_size=100,       # Dimensionality of the word vectors
                 window=5,            # Context window size
                 min_count=5,         # Minimum word frequency
                 workers=4,           # Number of worker threads
                 sg=1,              # Use Skip-gram (1) or CBOW (0)
                 negative=10,          # Number of negative samples (key parameter)
                 epochs=10)           # Number of training epochs</p>
<h1>3. Access Word Embeddings</h1>
<h1>Get the vector for a specific word</h1>
<p>word_vector = model.wv['king']  # Example: Get the vector for "king"
print(f"Vector for 'king': {word_vector}")</p>
<h1>Find similar words</h1>
<p>similar_words = model.wv.most_similar('king', topn=5)
print(f"Words similar to 'king': {similar_words}")</p>
<h1>4. Save and Load the Model</h1>
<h1>Save the trained model</h1>
<p>model.save("word2vec_brown_negative_sampling.model")</p>
<h1>Load the saved model</h1>
<p>loaded_model = Word2Vec.load("word2vec_brown_negative_sampling.model")</p>
<h1>Example usage with the loaded model:</h1>
<p>print(f"Vector for 'queen' (loaded model): {loaded_model.wv['queen']}")
print(f"Words similar to 'queen' (loaded model): {loaded_model.wv.most_similar('queen', topn=5)}")</p>
<p><strong>Explanation:</strong></p>
<ol>
<li><strong>Data Preparation:</strong> The code loads sentences from the NLTK Brown corpus (you would replace this with your own corpus). It converts all words to lowercase and tokenizes the sentences.</li>
<li><strong>Model Training:</strong>  The <code>Word2Vec</code> class from <code>gensim</code> is used to train the model.<ul>
<li><code>sentences</code>: The training data (list of lists of words).</li>
<li><code>vector_size</code>:  The dimensionality of the word vectors (e.g., 100, 300).  A higher dimension can capture more nuanced relationships but requires more data.</li>
<li><code>window</code>:  The maximum distance between the current and predicted word within a sentence.</li>
<li><code>min_count</code>: Ignores all words with total frequency lower than this.  This helps to remove rare words that may not contribute meaningfully to the embedding space.</li>
<li><code>workers</code>: Use these many worker threads to train the model (=faster training with multicore machines).</li>
<li><code>sg</code>: Training algorithm: 1 for skip-gram; otherwise CBOW.  Skip-gram tends to perform better with smaller datasets and better captures rare words.</li>
<li><code>negative</code>:  The key parameter! Specifies how many "noise words" should be drawn for each positive sample. A value between 5 and 20 is common for smaller datasets, and between 2 and 5 for large datasets.</li>
<li><code>epochs</code>: Number of iterations (epochs) over the corpus.</li>
</ul>
</li>
<li><strong>Accessing Embeddings:</strong> The <code>model.wv</code> attribute provides access to the word vectors. <code>model.wv['word']</code> returns the vector for a given word. <code>model.wv.most_similar('word')</code> returns a list of words most similar to the given word, based on cosine similarity.</li>
<li><strong>Saving/Loading:</strong> The trained model can be saved to disk and loaded later for use.</li>
</ol>
<p>4- Provide a follow up question about that topic</p>
<p><strong>Follow-up Question:</strong></p>
<p>How does the choice of the negative sampling distribution, <code>P_n(w)</code>, affect the quality and characteristics of the learned word embeddings, and are there scenarios where alternative distributions (other than the unigram distribution raised to the 3/4 power) might be more appropriate? Explain with some examples.</p>
</body>
</html>
