<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tokenization: Concepts and Challenges</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 24px;
      color: #111827;
      background: #ffffff;
    }
    h1, h2, h3 { color: #111827; }
    a { color: #2563eb; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code { background-color: #f3f4f6; padding: 2px 6px; border-radius: 6px; }
    pre { background-color: #0b1020; color: #e5e7eb; padding: 16px; border-radius: 10px; overflow-x: auto; }
    pre code { background: transparent; padding: 0; }
    blockquote { border-left: 4px solid #e5e7eb; margin: 0; padding-left: 16px; color: #4b5563; }
    table { width: 100%; border-collapse: collapse; margin: 16px 0; }
    th, td { border: 1px solid #e5e7eb; padding: 8px; text-align: left; }
    th { background: #f9fafb; }
    hr { border: none; border-top: 1px solid #e5e7eb; margin: 24px 0; }
  </style>
</head>
<body>
<h1 id="topic-tokenization-concepts-and-challenges">Topic: Tokenization: Concepts and Challenges</h1>
<h2 id="1-formal-definition-what-is-it-and-how-can-we-use-it">1) Formal definition (what is it, and how can we use it?)</h2>
<p>Tokenization is the process of breaking down a text string into smaller units called tokens. These tokens can be words, subwords, characters, or even symbols, depending on the chosen tokenization method.</p>
<p>Formally, given a string <em>S</em>, tokenization produces a sequence of tokens: <em>[t<sub>1</sub>, t<sub>2</sub>, ..., t<sub>n</sub>]</em> where <em>t<sub>i</sub></em> is a token and the concatenation of all <em>t<sub>i</sub></em> (possibly with added delimiters like spaces) reconstructs or approximates <em>S</em>.</p>
<p>We use tokenization as a crucial first step in many Natural Language Processing (NLP) tasks. It allows computers to process and analyze text by representing it as discrete units. These tokens can then be used for:</p>
<ul>
<li><strong>Text Representation:</strong> Converting text into a numerical format (e.g., using word embeddings or bag-of-words) which machine learning models can understand.</li>
<li><strong>Information Retrieval:</strong> Indexing documents based on tokens to enable efficient searching.</li>
<li><strong>Machine Translation:</strong> Breaking down sentences into tokens for translation.</li>
<li><strong>Sentiment Analysis:</strong> Identifying sentiment-bearing tokens.</li>
<li><strong>Language Modeling:</strong> Predicting the next token in a sequence.</li>
<li><strong>Parsing:</strong> Understanding the grammatical structure of a sentence.</li>
</ul>
<h2 id="2-application-scenario">2) Application scenario</h2>
<p>Imagine you are building a sentiment analysis system for movie reviews. A user enters the following review: "This movie was absolutely fantastic! The acting was superb, and the plot kept me engaged. Highly recommended!"</p>
<p>Before your sentiment analysis model can determine if the review is positive or negative, you need to tokenize it. A simple word tokenization might produce the following tokens:</p>
<p><code>["This", "movie", "was", "absolutely", "fantastic", "!", "The", "acting", "was", "superb", ",", "and", "the", "plot", "kept", "me", "engaged", ".", "Highly", "recommended", "!"]</code></p>
<p>These tokens can then be processed to remove punctuation, convert to lowercase, and used to calculate the overall sentiment score. More sophisticated tokenization methods might handle contractions ("wasn't" -&gt; "was", "n't") or use subword tokenization to handle unseen words (e.g., splitting "unseen" into "un" and "seen"). Without tokenization, the review would just be a single, unmanageable string.</p>
<h2 id="3-python-method-if-possible">3) Python method (if possible)</h2>
<pre class="codehilite"><code class="language-python">import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

# Download required NLTK data (only needs to be done once)
# nltk.download('punkt')  # Uncomment to download if you haven't already

text = &quot;This is a sentence. This is another sentence! Isn't this fun?&quot;

# Sentence tokenization
sentences = sent_tokenize(text)
print(&quot;Sentences:&quot;, sentences)

# Word tokenization
words = word_tokenize(text)
print(&quot;Words:&quot;, words)

# Word tokenization on a single sentence
single_sentence = sentences[0]
words_in_sentence = word_tokenize(single_sentence)
print(&quot;Words in single sentence:&quot;, words_in_sentence)

# Another example with special characters
text2 = &quot;Let's go to the U.S.A. today!&quot;
words2 = word_tokenize(text2)
print(&quot;Words with special characters:&quot;, words2)

# Using whitespace tokenizer (less common, but illustrates a point)
from nltk.tokenize import WhitespaceTokenizer
tokenizer = WhitespaceTokenizer()
words_whitespace = tokenizer.tokenize(text)
print(&quot;Words using WhitespaceTokenizer:&quot;, words_whitespace) #Notice the punctuation attached
</code></pre>

<h2 id="4-follow-up-question">4) Follow-up question</h2>
<p>What are some of the main challenges in tokenization, particularly when dealing with languages other than English, and how are different tokenization techniques designed to address those challenges? (Consider issues like compound words, agglutinative languages, and languages without explicit word separators).</p>
</body>
</html>
