
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Word2Vec (CBOW vs Skip-gram)</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
        }
        h1, h2, h3 { color: #2c3e50; }
        code { background-color: #f8f9fa; padding: 2px 4px; border-radius: 4px; }
        pre { background-color: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }
        blockquote { border-left: 4px solid #eee; margin: 0; padding-left: 15px; color: #666; }
    </style>
</head>
<body>
    <p>Topic: Introduction to Word2Vec (CBOW vs Skip-gram)</p>
<p>1- Provide formal definition, what is it and how can we use it?</p>
<p>Word2Vec is a group of models used to produce word embeddings. Word embeddings are vector representations of words that capture semantic relationships between them. These vectors are learned from large text corpora. The core idea is that words appearing in similar contexts should have similar vector representations. Essentially, it turns words into numbers (vectors) that a machine learning model can understand and that encode meaning.</p>
<p>There are two main architectures in Word2Vec:</p>
<ul>
<li>
<p><strong>Continuous Bag-of-Words (CBOW):</strong>  CBOW predicts a target word given the context words surrounding it. Formally, given context words <em>w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>c</sub></em>, CBOW aims to predict the target word <em>w<sub>t</sub></em>. The model averages the vector representations of the context words and then uses this average vector to predict the target word. The goal is to maximize the probability <em>P(w<sub>t</sub> | w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>c</sub>)</em>.</p>
</li>
<li>
<p><strong>Skip-gram:</strong> Skip-gram predicts the context words given a target word. Formally, given a target word <em>w<sub>t</sub></em>, Skip-gram aims to predict the surrounding context words <em>w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>c</sub></em>. The model tries to maximize the probability <em>P(w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>c</sub> | w<sub>t</sub>)</em>.</p>
</li>
</ul>
<p><strong>How can we use it?</strong></p>
<ul>
<li><strong>Semantic Similarity:</strong> Calculate the similarity between words. Words with similar meanings will have vectors that are close to each other in vector space (e.g., using cosine similarity).</li>
<li><strong>Analogy Completion:</strong> Solve analogy problems like "man is to king as woman is to ?" by using vector arithmetic (e.g.,  <em>vector('king') - vector('man') + vector('woman')</em> should be close to <em>vector('queen')</em>).</li>
<li><strong>Feature Engineering:</strong> Use word embeddings as features for various NLP tasks like text classification, sentiment analysis, machine translation, and named entity recognition. This is especially useful for models that can't directly process text (e.g., certain types of neural networks).</li>
<li><strong>Recommendation Systems:</strong> Represent items (e.g., products) using embeddings based on co-occurrence in user interactions, similar to how words co-occur in text.</li>
</ul>
<p>2- Provide an application scenario</p>
<p><strong>Application Scenario: Sentiment Analysis of Customer Reviews</strong></p>
<p>Imagine you want to build a sentiment analysis system to automatically classify customer reviews as positive, negative, or neutral. Instead of using traditional bag-of-words approaches, you can use Word2Vec to create word embeddings for the words in the reviews.</p>
<ol>
<li><strong>Train a Word2Vec model:</strong> Train a Word2Vec model (either CBOW or Skip-gram) on a large corpus of text data (e.g., Wikipedia, news articles, or even a collection of other reviews). This gives you vector representations for a large vocabulary of words.</li>
<li><strong>Embed the reviews:</strong> For each customer review, look up the word embeddings for each word in the review.</li>
<li><strong>Aggregate the word embeddings:</strong> Average the word embeddings of all words in a review to create a single vector representation for the entire review. Alternatively, more sophisticated methods like using RNNs or Transformers to combine the word embeddings can be employed.</li>
<li><strong>Train a classifier:</strong> Train a machine learning classifier (e.g., logistic regression, support vector machine, or a neural network) using the aggregated review embeddings as features and the sentiment labels (positive, negative, neutral) as the target variable.</li>
</ol>
<p>By using Word2Vec embeddings, the sentiment analysis system can capture semantic relationships between words, allowing it to better understand the context and meaning of the reviews. For example, it can recognize that "great" and "fantastic" are semantically similar and contribute positively to the sentiment.</p>
<p>3- Provide a method to apply in python</p>
<p>python
import gensim
from gensim.models import Word2Vec
from nltk.corpus import brown # Example corpus
from nltk.tokenize import word_tokenize</p>
<h1>Download brown corpus if you haven't already</h1>
<p>import nltk
try:
    brown.words()
except LookupError:
    nltk.download('brown')
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')</p>
<h1>1. Prepare the data: Tokenize sentences in the corpus</h1>
<p>sentences = brown.sents() # Already tokenized into sentences</p>
<h1>2. Train the Word2Vec model</h1>
<p>model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=0)  # CBOW by default</p>
<h1>vector_size: Dimensionality of the word vectors</h1>
<h1>window: Maximum distance between the current and predicted word within a sentence</h1>
<h1>min_count: Ignores all words with total frequency lower than this</h1>
<h1>workers: Use these many worker threads to train the model (=faster training with multicore machines).</h1>
<h1>sg: Training algorithm: 1 for skip-gram; otherwise, CBOW.</h1>
<h1>To train Skip-gram model, change sg=1</h1>
<h1>model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=1)</h1>
<h1>3. Access word vectors</h1>
<p>vector = model.wv['king'] # Get the vector representation of the word 'king'
print(f"Vector for 'king': {vector[:10]}...") # Print the first 10 elements</p>
<h1>4. Find similar words</h1>
<p>similar_words = model.wv.most_similar('king', topn=5) # Find the 5 most similar words to 'king'
print(f"Similar words to 'king': {similar_words}")</p>
<h1>5. Save and load the model</h1>
<p>model.save("word2vec.model")
loaded_model = Word2Vec.load("word2vec.model")</p>
<h1>Example usage after loading the model</h1>
<p>vector_loaded = loaded_model.wv['king']
print(f"Vector for 'king' from loaded model: {vector_loaded[:10]}...")</p>
<h1>Clean up the saved model file (optional)</h1>
<p>import os
os.remove("word2vec.model")</p>
<p><strong>Explanation:</strong></p>
<ol>
<li><strong>Import Libraries:</strong> Import the necessary libraries: <code>gensim</code> for Word2Vec and <code>nltk</code> for text processing (Brown corpus).</li>
<li><strong>Prepare Data:</strong> The Brown corpus is used as an example text corpus. The <code>brown.sents()</code> function provides sentences already tokenized into words.</li>
<li><strong>Train the Model:</strong><ul>
<li><code>Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4, sg=0)</code>:  This line creates and trains the Word2Vec model.<ul>
<li><code>sentences</code>: The training data (list of sentences, where each sentence is a list of words).</li>
<li><code>vector_size</code>: The dimensionality of the word vectors (e.g., 100 dimensions).  A higher dimensionality can capture more complex semantic relationships, but it also increases the computational cost.</li>
<li><code>window</code>: The maximum distance between a target word and words around the target word.  A larger window captures more context.</li>
<li><code>min_count</code>:  Words that appear less than <code>min_count</code> times in the corpus are ignored.  This helps to remove rare words and improve the quality of the embeddings.</li>
<li><code>workers</code>:  The number of worker threads to use for training.  This can speed up training on multi-core machines.</li>
<li><code>sg</code>:  Determines the training algorithm: <code>0</code> for CBOW (default), <code>1</code> for Skip-gram.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Access Word Vectors:</strong> <code>model.wv['king']</code> retrieves the vector representation of the word "king".</li>
<li><strong>Find Similar Words:</strong> <code>model.wv.most_similar('king', topn=5)</code> finds the 5 words most similar to "king" based on cosine similarity.</li>
<li><strong>Save and Load the Model:</strong>  The trained model is saved to disk using <code>model.save("word2vec.model")</code> and loaded using <code>Word2Vec.load("word2vec.model")</code>. Saving the model allows you to reuse it later without retraining.</li>
</ol>
<p>4- Provide a follow up question about that topic</p>
<p>How does the choice of training corpus size, vector size, window size, and min_count affect the quality of the resulting word embeddings, and what are some strategies for tuning these hyperparameters to optimize performance for a specific downstream task?</p>
</body>
</html>
